{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load('pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition\n",
    "\n",
    "Named Entities supported by Spacy: https://spacy.io/api/annotation#named-entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maria PER\n",
      "Paris LOC\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('Maria estÃ¡ se mudando para Paris. No dia 01/02/2019 ela irÃ¡ partir.')\n",
    "for entidade in doc.ents:\n",
    "    print(entidade.text, entidade.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Maria\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " estÃ¡ se mudando para \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Paris\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ". No dia 01/02/2019 ela irÃ¡ partir.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Named person or family.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('PER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model 'pt'\n",
      "Losses {'ner': 30.92538600668653}\n",
      "Losses {'ner': 31.454840575629262}\n",
      "Losses {'ner': 27.404309965555072}\n",
      "Losses {'ner': 28.070894540207064}\n",
      "Losses {'ner': 22.631297555781202}\n",
      "Losses {'ner': 23.80390477474384}\n",
      "Losses {'ner': 22.869779231175926}\n",
      "Losses {'ner': 19.038161833159393}\n",
      "Losses {'ner': 23.323969737626612}\n",
      "Losses {'ner': 21.236340762348846}\n",
      "Losses {'ner': 20.657431311672553}\n",
      "Losses {'ner': 21.386681608855724}\n",
      "Losses {'ner': 18.974342360161245}\n",
      "Losses {'ner': 24.957486101659015}\n",
      "Losses {'ner': 19.19161887653172}\n",
      "Losses {'ner': 12.495212723846635}\n",
      "Losses {'ner': 26.037852619716432}\n",
      "Losses {'ner': 15.336943093978334}\n",
      "Losses {'ner': 18.387670449679717}\n",
      "Losses {'ner': 14.352215200517094}\n",
      "Losses {'ner': 21.485799086629413}\n",
      "Losses {'ner': 18.102753300452605}\n",
      "Losses {'ner': 20.468476392285083}\n",
      "Losses {'ner': 19.64151975273853}\n",
      "Losses {'ner': 16.078637087732204}\n",
      "Losses {'ner': 18.903074446301616}\n",
      "Losses {'ner': 16.963952124460775}\n",
      "Losses {'ner': 14.217290301407047}\n",
      "Losses {'ner': 15.036651713206084}\n",
      "Losses {'ner': 20.84510894727555}\n",
      "Entities in 'quero meu relatÃ³rio de vendas entre 01/02/2020 e 01/03/2020'\n",
      "MISC quero\n",
      "DATE 01/02/2020\n",
      "DATE 01/03/2020\n",
      "Saved model to modelo_kodama.md\n",
      "Loading from modelo_kodama.md\n",
      "MISC quero\n",
      "DATE 01/02/2020\n",
      "DATE 01/03/2020\n"
     ]
    }
   ],
   "source": [
    "# treinar o modelo com os novos dados\n",
    "!python train_new_entity_type.py -m pt -o modelo_kodama.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('./modelo_kodama.md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    quero\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n",
       "</mark>\n",
       " meu relatÃ³rio de vendas entre \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    01/02/2020\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " e \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    01/03/2020\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp('quero meu relatÃ³rio de vendas entre 01/02/2020 e 01/03/2020')\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NÃ³s podemos observar que com um modelo probabislÃ­tico temos alguns problemas em identificar datas. Precisamos de mais exemplos e talvez nesse caso seja mais interessante utilizar uma abordagem baseda em regras (regex) e no caso de dados relativas como \"ultimo mes\" \"semestre de 2020\" podemos utilizar uma abordagem combinada com regras e bases de conhecimento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule based and Knowledge Based\n",
    "\n",
    "https://spacy.io/usage/rule-based-matching\n",
    "https://spacy.io/usage/rule-based-matching#entityruler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ultimo mÃªs', 'DATE')]\n",
      "[('Ãºltimos 3 meses', 'DATE')]\n",
      "[('agosto', 'DATE')]\n",
      "[('marÃ§o', 'DATE'), ('junho', 'DATE')]\n",
      "[('Maio', 'DATE'), ('Agosto', 'DATE')]\n",
      "[('abril', 'DATE'), ('junho', 'DATE')]\n",
      "[('hoje', 'DATE')]\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.pt import Portuguese\n",
    "from spacy.pipeline import EntityRuler\n",
    "\n",
    "nlp = Portuguese() # if you want to combina with existing prob model use spacy.load('pt')\n",
    "ruler = EntityRuler(nlp)\n",
    "patterns = [\n",
    "    {\n",
    "        'label': 'DATE', 'pattern': \n",
    "        [    # this will run per token and check the token sequence to match the rules\n",
    "            {'TEXT' : {\"REGEX\": \"[uÃº]ltimo\"} },\n",
    "            {'TEXT' : {\"REGEX\": \"m[Ãªe]s\"} },\n",
    "        ], \n",
    "        'id': 'date'\n",
    "    },\n",
    "    {\n",
    "        'label': 'DATE', 'pattern': \n",
    "        [\n",
    "            {'TEXT' : {\"REGEX\": \"[uÃº]ltimo\"} },\n",
    "            {'TEXT' : {\"REGEX\": \"\\d\"} },\n",
    "            {'TEXT' : {\"REGEX\": \"m[Ãªe]s\"} },\n",
    "        ], \n",
    "        'id': 'date'\n",
    "    },\n",
    "    {\n",
    "        'label': 'DATE', 'pattern': \n",
    "        [\n",
    "            {'TEXT' : {\"REGEX\": \"([Jj]aneiro|[Ff]evereiro|[Mm]ar[Ã§c]o|[Aa]bril|[Mm]aio|[Jj]unho|[Jj]ulho|[Aa]gosto|[Ss]etembro|[Oo]utubro|[Nn]ovembro|[Dd]ezembro)\"} }\n",
    "        ], \n",
    "        'id': 'date'\n",
    "    }, {\n",
    "        'label': 'DATE', 'pattern': \n",
    "        [\n",
    "            {'TEXT' : {\"REGEX\": \"(hoje|ontem)\"} }\n",
    "        ], \n",
    "        'id': 'date'\n",
    "    }\n",
    "]\n",
    "ruler.add_patterns(patterns)\n",
    "nlp.add_pipe(ruler)\n",
    "\n",
    "messages = [\n",
    "    'quero relatÃ³rio de vendas do ultimo mÃªs',\n",
    "    'como foram as minhas vendas nos Ãºltimos 3 meses',\n",
    "    'vendas do mÃªs de agosto',\n",
    "    'minhas vendas do mes de marÃ§o atÃ© junho',\n",
    "    'como foram as minhas vendas entre Maio e Agosto',\n",
    "    'quanto vendi entre abril e junho?',\n",
    "    'qual produto eu mais vendi hoje'\n",
    "]\n",
    "\n",
    "def test_date_entity(messages):\n",
    "    for message in messages:\n",
    "        doc = nlp(message)\n",
    "        print([(ent.text, ent.label_) for ent in doc.ents])\n",
    "\n",
    "test_date_entity(messages)\n",
    "\n",
    "# nlp.to_disk(\"/path/to/model\") saving model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.lang.pt import Portuguese\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.metrics.distance import edit_distance\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "class Preprocessing:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def remove_stop_words(self, message, tokenizer, tokenized = True):\n",
    "        blacklist = set(stopwords.words('portuguese') + list(punctuation))\n",
    "        tokens = [] \n",
    "        if tokenizer:\n",
    "            tokens = tokenizer(message)\n",
    "        else:\n",
    "            tokens = word_tokenize(message)\n",
    "        clean_words = [word for word in tokens if word not in blacklist]\n",
    "        if tokenized:\n",
    "            return clean_words\n",
    "        else:\n",
    "            return ' '.join(clean_words)\n",
    "        \n",
    "        \n",
    "\n",
    "class NER:\n",
    "    '''\n",
    "     NER Approaches created by Mee ðŸ¤–ðŸ”®ðŸ’œ\n",
    "     Author: Guilherme Kodama 07/2020\n",
    "    '''\n",
    "    def __init__(self, corpus):\n",
    "        self.preprocessing = Preprocessing()\n",
    "        self.nlp = Portuguese()\n",
    "        self.vectorizer = CountVectorizer()\n",
    "        self.matrix = self.vectorizer.fit_transform(corpus)\n",
    "        self.vocab = self.vectorizer.vocabulary_\n",
    "        self.tokenizer = self.vectorizer.build_tokenizer()\n",
    "        \n",
    "    def recognize(self, message, algo='perfect_match', threshold=2):\n",
    "        tokens = self.preprocessing.remove_stop_words(message, tokenizer=self.tokenizer)\n",
    "\n",
    "        for token in tokens:\n",
    "            if algo == 'perfect_match':\n",
    "                if token in self.vocab:\n",
    "                    print(token)\n",
    "            # Calculate the Levenshtein edit-distance https://www.nltk.org/api/nltk.metrics.html#nltk.metrics.distance.edit_distance\n",
    "            elif algo == 'edit_distance':\n",
    "                for word in self.vectorizer.get_feature_names():\n",
    "                    if edit_distance(token, word) <= threshold:\n",
    "                        print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coca\n",
      "coca\n",
      "colas\n",
      "colas\n"
     ]
    }
   ],
   "source": [
    "ner = NER(['parmegiana de frango', 'bife a parmegiana', 'refrigerante coca-cola', 'heineken 600ml'])\n",
    "ner.vectorizer.vocabulary_\n",
    "ner.recognize('quantas coca-colas tenho no estoque ?', algo='edit_distance')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
