{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify Intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "from spacy import displacy\n",
    "from string import punctuation\n",
    "from nltk.stem import RSLPStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.pipeline import EntityRuler\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([\n",
    "     # sales\n",
    "     ['vendas do Ãºltimo mÃªs', 'sales'],\n",
    "     ['quanto vendi esse mÃªs', 'sales'],\n",
    "     ['quanto vendi no mÃªs de agosto', 'sales'],\n",
    "     ['quais sÃ£o os 10 produtos que mais vendi esse ano', 'sales'],\n",
    "     ['quais sÃ£o os 10 produtos que mais vendi esse mÃªs', 'sales'],\n",
    "     ['quais sÃ£o os 10 produtos que mais vendi hoje', 'sales'],\n",
    "     ['qual produto eu mais vendi hoje', 'sales'],\n",
    "     # products\n",
    "     ['quantos produtos eu tenho na minha base', 'products'],\n",
    "     ['quantos produtos eu tenho na minha base de dados', 'products'],\n",
    "     ['quantos produtos eu tenho na minha loja', 'products'],\n",
    "     ['quantos refrigerantes eu vendo na minha loja', 'products'],\n",
    "     ['quanto Ã© o preÃ§o da coca-cola', 'products'],\n",
    "     ['qual Ã© o preÃ§o da coca-cola', 'products'],\n",
    "     ['quantos refrigerantes tenho no estoque', 'products']\n",
    " ], columns=['message', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatBot:\n",
    "    '''\n",
    "     Business Chat Bot by Mee ðŸ¤–ðŸ”®ðŸ’œ\n",
    "     Author: Guilherme Kodama 06/2020\n",
    "    '''\n",
    "    def __init__(self, df, column_message='message', column_label='label'):\n",
    "        self.df = df.copy()\n",
    "        self.clf = MultinomialNB()\n",
    "        self.nlp = spacy.load('pt')\n",
    "        self.stemmer = RSLPStemmer()\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.ruler = EntityRuler(nlp)\n",
    "        \n",
    "        # custom pattern for entity recognition\n",
    "        self.patterns = [\n",
    "            {\n",
    "                'label': 'DATE', 'pattern': \n",
    "                [    # this will run per token and check the token sequence to match the rules\n",
    "                    {'TEXT' : {\"REGEX\": \"[uÃº]ltimo\"} },\n",
    "                    {'TEXT' : {\"REGEX\": \"m[Ãªe]s\"} },\n",
    "                ], \n",
    "                'id': 'date'\n",
    "            },\n",
    "            {\n",
    "                'label': 'DATE', 'pattern': \n",
    "                [\n",
    "                    {'TEXT' : {\"REGEX\": \"[uÃº]ltimo\"} },\n",
    "                    {'TEXT' : {\"REGEX\": \"\\d\"} },\n",
    "                    {'TEXT' : {\"REGEX\": \"m[Ãªe]s\"} },\n",
    "                ], \n",
    "                'id': 'date'\n",
    "            },\n",
    "            {\n",
    "                'label': 'DATE', 'pattern': \n",
    "                [\n",
    "                    {'TEXT' : {\"REGEX\": \"([Jj]aneiro|[Ff]evereiro|[Mm]ar[Ã§c]o|[Aa]bril|[Mm]aio|[Jj]unho|[Jj]ulho|[Aa]gosto|[Ss]etembro|[Oo]utubro|[Nn]ovembro|[Dd]ezembro)\"} }\n",
    "                ], \n",
    "                'id': 'date'\n",
    "            }, {\n",
    "                'label': 'DATE', 'pattern': \n",
    "                [\n",
    "                    {'TEXT' : {\"REGEX\": \"(hoje|ontem)\"} }\n",
    "                ], \n",
    "                'id': 'date'\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        self.ruler.add_patterns(patterns)\n",
    "        self.nlp.add_pipe(ruler)\n",
    "    \n",
    "    def understand(self, message):\n",
    "        print('--- INTENT ---')\n",
    "        print('')\n",
    "        self.train()\n",
    "        prediction = self.predict(message)\n",
    "        print('PREDICTION: ', prediction)\n",
    "        print('')\n",
    "        print('--- ENTITIES ---')\n",
    "        print('')\n",
    "        self.extract_entities(message, '')\n",
    "    \n",
    "    '''\n",
    "     PREPROCESSING TEXT\n",
    "    '''\n",
    "\n",
    "    def normalize(self, message):\n",
    "        normalized = unicodedata.normalize('NFKD', message).encode('ASCII', 'ignore').decode('utf-8')\n",
    "        return normalized\n",
    "\n",
    "    def stem(self, message):\n",
    "        words = word_tokenize(message)\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "        return ' '.join(words)\n",
    "\n",
    "    def remove_stopwords(self, message):\n",
    "        blacklist = set(stopwords.words('portuguese') + list(punctuation))\n",
    "        clean_words = [word for word in word_tokenize(message) if word not in blacklist]\n",
    "        return ' '.join(clean_words)\n",
    "\n",
    "    def preprocess_message(self, message):\n",
    "        message = self.normalize(message)\n",
    "        message = self.remove_stopwords(message)\n",
    "    #     message = stem(message)\n",
    "        return message\n",
    "\n",
    "    def preprocess(self, df, column_in='message', column_out='message_clean'):\n",
    "        df[column_out] = df[column_in].apply(lambda message : self.preprocess_message(message))\n",
    "\n",
    "    '''\n",
    "     VECTORIZE\n",
    "    '''\n",
    "    def vectorize(self, df, column = 'message'):\n",
    "        X = vectorizer.fit_transform(df[column])\n",
    "        return X\n",
    "\n",
    "    def vectorize_message(self, message):\n",
    "        return vectorizer.transform([message])\n",
    "    '''\n",
    "     PREDICTION\n",
    "    '''\n",
    "    def train(self):\n",
    "        self.preprocess(self.df, column_in='message', column_out='message_clean')\n",
    "        self.X = vectorize(self.df, column='message_clean')\n",
    "        print('CORPUS:', vectorizer.get_feature_names())\n",
    "        print('')\n",
    "        print('CORPUS SHAPE: ', X.shape)\n",
    "        print('')\n",
    "        self.model = clf.fit(self.X, self.df['label'])\n",
    "\n",
    "    def predict(self, message):\n",
    "        vector = vectorize_message(message)\n",
    "        predict_label = self.model.predict(vector)\n",
    "        predict_proba = self.model.predict_proba(vector)\n",
    "        return { 'label': predict_label, 'probability': predict_proba, 'classes': model.classes_ }\n",
    "\n",
    "    '''\n",
    "     NER - Named Entity Recognition\n",
    "    '''\n",
    "\n",
    "    def extract_entities(self, message, intent):\n",
    "        doc = self.nlp(message)\n",
    "        print([(ent.text, ent.label_) for ent in doc.ents])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot = ChatBot(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot.understand('qual produto eu mais vendi hoje')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
